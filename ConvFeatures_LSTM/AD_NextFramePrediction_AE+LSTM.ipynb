{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_feature_extraction import ImageFeatureExtractor\n",
    "from train_frame_prediction import FramePredictor_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext = ImageFeatureExtractor(useGPU = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoders.C2D_Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C2D_AE_128_3x3(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = C2D_AE_128_3x3(channels=1)\n",
    "load_model(model, \"../AutoEncoders/C2D_AE_models/C2D_AE_128_3x3_UCSD2/C2D_AE_128_3x3_UCSD2.pth.tar\")\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext.feature_extractor = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        return feat_ext.feature_extractor(images.to(feat_ext.device))[-1].detach().cpu().flatten(start_dim = 1, end_dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:47,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "ucsd_train = UCSD(2, asImages = False, image_size=128, n_frames=16, sample_stride = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucsd_processed_train = [(extract_features(data.transpose(0,1)), label) for (data, label) in ucsd_train]\n",
    "train_loader, val_loader = get_data_loaders(ucsd_processed_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del trainer\n",
    "except: pass\n",
    "trainer = FramePredictor_Trainer(256, 256, useGPU=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_prediction import FrameFeaturePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = FrameFeaturePredictor(256,256, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/201] Train Loss: 39065.8283 | Val Loss: 40465.2651\n",
      "[2/201] Train Loss: 34462.6278 | Val Loss: 39110.8061\n",
      "[3/201] Train Loss: 32570.9159 | Val Loss: 37408.8149\n",
      "[4/201] Train Loss: 30775.2231 | Val Loss: 35844.9364\n",
      "[5/201] Train Loss: 29321.4285 | Val Loss: 34647.7466\n",
      "[6/201] Train Loss: 28091.0542 | Val Loss: 33441.8020\n",
      "[7/201] Train Loss: 27026.7880 | Val Loss: 32282.4510\n",
      "[8/201] Train Loss: 26085.5718 | Val Loss: 31316.6323\n",
      "[9/201] Train Loss: 25224.0602 | Val Loss: 30272.2868\n",
      "[10/201] Train Loss: 24439.3512 | Val Loss: 29325.3858\n",
      "[11/201] Train Loss: 23716.0106 | Val Loss: 28512.4901\n",
      "[12/201] Train Loss: 23055.0952 | Val Loss: 27649.7342\n",
      "[13/201] Train Loss: 22440.9405 | Val Loss: 27054.6278\n",
      "[14/201] Train Loss: 21877.0072 | Val Loss: 26479.8816\n",
      "[15/201] Train Loss: 21358.4737 | Val Loss: 25954.4524\n",
      "[16/201] Train Loss: 20880.3579 | Val Loss: 25513.2650\n",
      "[17/201] Train Loss: 20440.3754 | Val Loss: 24978.5609\n",
      "[18/201] Train Loss: 20031.7005 | Val Loss: 24650.3381\n",
      "[19/201] Train Loss: 19659.5570 | Val Loss: 24449.1250\n",
      "[20/201] Train Loss: 19315.0566 | Val Loss: 24075.6705\n",
      "[21/201] Train Loss: 18996.6621 | Val Loss: 23782.2069\n",
      "[22/201] Train Loss: 18704.9784 | Val Loss: 23605.0167\n",
      "[23/201] Train Loss: 18436.4632 | Val Loss: 23308.1590\n",
      "[24/201] Train Loss: 18189.5848 | Val Loss: 23393.2765\n",
      "[25/201] Train Loss: 17962.7478 | Val Loss: 23101.3287\n",
      "[26/201] Train Loss: 17755.1965 | Val Loss: 22991.5748\n",
      "[27/201] Train Loss: 17561.2353 | Val Loss: 22779.4098\n",
      "[28/201] Train Loss: 17385.2589 | Val Loss: 22512.9150\n",
      "[29/201] Train Loss: 17221.2529 | Val Loss: 22515.3564\n",
      "[30/201] Train Loss: 17076.3273 | Val Loss: 22530.5478\n",
      "[31/201] Train Loss: 16938.1583 | Val Loss: 22442.7615\n",
      "[32/201] Train Loss: 16810.5281 | Val Loss: 22391.3034\n",
      "[33/201] Train Loss: 16696.4733 | Val Loss: 22245.9476\n",
      "[34/201] Train Loss: 16589.2174 | Val Loss: 22293.4906\n",
      "[35/201] Train Loss: 16492.7070 | Val Loss: 22082.9451\n",
      "[36/201] Train Loss: 16403.2099 | Val Loss: 22009.9407\n",
      "[37/201] Train Loss: 16318.1556 | Val Loss: 21958.5202\n",
      "[38/201] Train Loss: 16239.8010 | Val Loss: 22178.9334\n",
      "[39/201] Train Loss: 16171.5284 | Val Loss: 21877.3249\n",
      "[40/201] Train Loss: 16102.4730 | Val Loss: 21835.6713\n",
      "[41/201] Train Loss: 16041.3300 | Val Loss: 21973.6522\n",
      "[42/201] Train Loss: 15983.8313 | Val Loss: 21922.5128\n",
      "[43/201] Train Loss: 15931.9387 | Val Loss: 21858.2033\n",
      "[44/201] Train Loss: 15877.0166 | Val Loss: 21904.5149\n",
      "[45/201] Train Loss: 15816.9165 | Val Loss: 22008.6856\n",
      "[46/201] Train Loss: 15766.9154 | Val Loss: 21771.0458\n",
      "[47/201] Train Loss: 15729.6594 | Val Loss: 21675.8116\n",
      "[48/201] Train Loss: 15700.9304 | Val Loss: 21728.5888\n",
      "[49/201] Train Loss: 15670.3233 | Val Loss: 21755.1543\n",
      "[50/201] Train Loss: 15638.2206 | Val Loss: 21715.0795\n",
      "[51/201] Train Loss: 15612.6154 | Val Loss: 21794.2335\n",
      "[52/201] Train Loss: 15578.4448 | Val Loss: 21794.4486\n",
      "[53/201] Train Loss: 15542.0139 | Val Loss: 21611.1557\n",
      "[54/201] Train Loss: 15515.7575 | Val Loss: 21648.0541\n",
      "[55/201] Train Loss: 15494.8956 | Val Loss: 21651.9338\n",
      "[56/201] Train Loss: 15474.1746 | Val Loss: 21689.3025\n",
      "[57/201] Train Loss: 15452.5115 | Val Loss: 21760.5670\n",
      "[58/201] Train Loss: 15434.2715 | Val Loss: 21509.4450\n",
      "[59/201] Train Loss: 15413.7033 | Val Loss: 21556.2075\n",
      "[60/201] Train Loss: 15396.6171 | Val Loss: 21947.9640\n",
      "[61/201] Train Loss: 15377.7179 | Val Loss: 21684.3450\n",
      "[62/201] Train Loss: 15360.0706 | Val Loss: 21669.9980\n",
      "[63/201] Train Loss: 15343.5540 | Val Loss: 21761.9367\n",
      "[64/201] Train Loss: 15326.0423 | Val Loss: 21707.4256\n",
      "[65/201] Train Loss: 15313.7922 | Val Loss: 21551.9505\n",
      "[66/201] Train Loss: 15300.3079 | Val Loss: 21764.8404\n",
      "[67/201] Train Loss: 15289.4008 | Val Loss: 21670.0106\n",
      "[68/201] Train Loss: 15278.0884 | Val Loss: 21729.9122\n",
      "[69/201] Train Loss: 15264.9125 | Val Loss: 21634.5745\n",
      "[70/201] Train Loss: 15256.9288 | Val Loss: 21707.1953\n",
      "[71/201] Train Loss: 15245.9347 | Val Loss: 21685.5766\n",
      "[72/201] Train Loss: 15237.9589 | Val Loss: 21587.6792\n",
      "[73/201] Train Loss: 15228.0965 | Val Loss: 21663.0962\n",
      "[74/201] Train Loss: 15220.6583 | Val Loss: 21564.2429\n",
      "[75/201] Train Loss: 15213.7731 | Val Loss: 21633.7114\n",
      "[76/201] Train Loss: 15205.0041 | Val Loss: 21697.5519\n",
      "[77/201] Train Loss: 15200.0987 | Val Loss: 21600.0421\n",
      "[78/201] Train Loss: 15191.8745 | Val Loss: 21629.7909\n",
      "[79/201] Train Loss: 15185.5247 | Val Loss: 21706.0777\n",
      "[80/201] Train Loss: 15181.6885 | Val Loss: 21707.6787\n",
      "[81/201] Train Loss: 15175.7537 | Val Loss: 21631.1287\n",
      "[82/201] Train Loss: 15170.3457 | Val Loss: 21707.1220\n",
      "[83/201] Train Loss: 15167.0140 | Val Loss: 21684.5071\n",
      "[84/201] Train Loss: 15160.5370 | Val Loss: 21742.7896\n",
      "[85/201] Train Loss: 15156.1475 | Val Loss: 21719.3459\n",
      "[86/201] Train Loss: 15154.4833 | Val Loss: 21681.2436\n",
      "[87/201] Train Loss: 15149.2221 | Val Loss: 21688.4302\n",
      "[88/201] Train Loss: 15144.6415 | Val Loss: 21794.0636\n",
      "[89/201] Train Loss: 15141.4354 | Val Loss: 21787.0969\n",
      "[90/201] Train Loss: 15137.4346 | Val Loss: 21744.3719\n",
      "[91/201] Train Loss: 15134.9356 | Val Loss: 21698.7705\n",
      "[92/201] Train Loss: 15132.1832 | Val Loss: 21786.1309\n",
      "[93/201] Train Loss: 15130.9236 | Val Loss: 21779.1785\n",
      "[94/201] Train Loss: 15126.7395 | Val Loss: 21773.9355\n",
      "[95/201] Train Loss: 15124.4990 | Val Loss: 21752.5467\n",
      "[96/201] Train Loss: 15122.6706 | Val Loss: 21742.1425\n",
      "[97/201] Train Loss: 15119.8918 | Val Loss: 21766.0189\n",
      "[98/201] Train Loss: 15117.5275 | Val Loss: 21755.3973\n",
      "[99/201] Train Loss: 15114.0470 | Val Loss: 21804.1341\n",
      "[100/201] Train Loss: 15112.5083 | Val Loss: 21776.1659\n",
      "[101/201] Train Loss: 15112.2219 | Val Loss: 21829.5792\n",
      "[102/201] Train Loss: 15111.2029 | Val Loss: 21768.1034\n",
      "[103/201] Train Loss: 15109.4367 | Val Loss: 21776.3010\n",
      "[104/201] Train Loss: 15106.5065 | Val Loss: 21775.8042\n",
      "[105/201] Train Loss: 15104.7832 | Val Loss: 21719.7674\n",
      "[106/201] Train Loss: 15105.6813 | Val Loss: 21810.1784\n",
      "[107/201] Train Loss: 15103.9475 | Val Loss: 21739.1009\n",
      "[108/201] Train Loss: 15102.1054 | Val Loss: 21746.2825\n",
      "[109/201] Train Loss: 15102.1426 | Val Loss: 21787.1994\n",
      "[110/201] Train Loss: 15100.2209 | Val Loss: 21777.6521\n",
      "[111/201] Train Loss: 15100.5165 | Val Loss: 21766.2748\n",
      "[112/201] Train Loss: 15098.7569 | Val Loss: 21800.1346\n",
      "[113/201] Train Loss: 15098.3998 | Val Loss: 21759.0326\n",
      "[114/201] Train Loss: 15096.4966 | Val Loss: 21764.1016\n",
      "[115/201] Train Loss: 15097.1872 | Val Loss: 21752.8737\n",
      "[116/201] Train Loss: 15094.6331 | Val Loss: 21780.4399\n",
      "[117/201] Train Loss: 15093.2925 | Val Loss: 21783.0455\n",
      "[118/201] Train Loss: 15092.9222 | Val Loss: 21785.5684\n",
      "[119/201] Train Loss: 15092.4864 | Val Loss: 21747.1875\n",
      "[120/201] Train Loss: 15093.0381 | Val Loss: 21810.2676\n",
      "[121/201] Train Loss: 15092.2901 | Val Loss: 21754.8153\n",
      "[122/201] Train Loss: 15088.8877 | Val Loss: 21789.1726\n",
      "[123/201] Train Loss: 15090.8117 | Val Loss: 21752.8490\n",
      "[124/201] Train Loss: 15089.6853 | Val Loss: 21765.9300\n",
      "[125/201] Train Loss: 15089.9481 | Val Loss: 21759.0181\n",
      "[126/201] Train Loss: 15089.1746 | Val Loss: 21777.2562\n",
      "[127/201] Train Loss: 15089.2812 | Val Loss: 21767.3864\n",
      "[128/201] Train Loss: 15087.9548 | Val Loss: 21785.2037\n",
      "[129/201] Train Loss: 15088.1221 | Val Loss: 21783.5178\n",
      "[130/201] Train Loss: 15087.7741 | Val Loss: 21790.7413\n",
      "[131/201] Train Loss: 15088.3152 | Val Loss: 21752.5822\n",
      "[132/201] Train Loss: 15087.0395 | Val Loss: 21787.7671\n",
      "[133/201] Train Loss: 15087.5855 | Val Loss: 21776.1570\n",
      "[134/201] Train Loss: 15087.0227 | Val Loss: 21772.8977\n",
      "[135/201] Train Loss: 15084.5847 | Val Loss: 21769.6577\n",
      "[136/201] Train Loss: 15086.2280 | Val Loss: 21779.2379\n",
      "[137/201] Train Loss: 15086.1634 | Val Loss: 21776.8051\n",
      "[138/201] Train Loss: 15086.3248 | Val Loss: 21782.2709\n",
      "[139/201] Train Loss: 15086.1893 | Val Loss: 21773.7152\n",
      "[140/201] Train Loss: 15085.2419 | Val Loss: 21783.5149\n",
      "[141/201] Train Loss: 15086.2455 | Val Loss: 21784.8361\n",
      "[142/201] Train Loss: 15083.3770 | Val Loss: 21774.9059\n",
      "[143/201] Train Loss: 15085.1063 | Val Loss: 21775.5103\n",
      "[144/201] Train Loss: 15084.9198 | Val Loss: 21779.6890\n",
      "[145/201] Train Loss: 15084.1216 | Val Loss: 21776.9572\n",
      "[146/201] Train Loss: 15083.6592 | Val Loss: 21779.8263\n",
      "[147/201] Train Loss: 15084.4593 | Val Loss: 21777.5062\n",
      "[148/201] Train Loss: 15083.3098 | Val Loss: 21778.5309\n",
      "[149/201] Train Loss: 15083.3849 | Val Loss: 21783.3510\n",
      "[150/201] Train Loss: 15084.5531 | Val Loss: 21783.8651\n",
      "[151/201] Train Loss: 15084.5359 | Val Loss: 21782.9703\n",
      "[152/201] Train Loss: 15083.9452 | Val Loss: 21781.0083\n",
      "[153/201] Train Loss: 15082.1695 | Val Loss: 21776.9601\n",
      "[154/201] Train Loss: 15084.4582 | Val Loss: 21791.0547\n",
      "[155/201] Train Loss: 15083.9604 | Val Loss: 21779.3515\n",
      "[156/201] Train Loss: 15084.9529 | Val Loss: 21774.0871\n",
      "[157/201] Train Loss: 15084.6647 | Val Loss: 21778.6175\n",
      "[158/201] Train Loss: 15083.3191 | Val Loss: 21784.8185\n",
      "[159/201] Train Loss: 15083.7668 | Val Loss: 21771.6448\n",
      "[160/201] Train Loss: 15084.1005 | Val Loss: 21778.8358\n",
      "[161/201] Train Loss: 15083.7078 | Val Loss: 21776.2758\n",
      "[162/201] Train Loss: 15083.6462 | Val Loss: 21784.1286\n",
      "[163/201] Train Loss: 15083.0394 | Val Loss: 21784.1072\n",
      "[164/201] Train Loss: 15082.5650 | Val Loss: 21775.1788\n",
      "[165/201] Train Loss: 15082.3703 | Val Loss: 21780.3089\n",
      "[166/201] Train Loss: 15083.6590 | Val Loss: 21780.2225\n",
      "[167/201] Train Loss: 15083.9275 | Val Loss: 21785.1479\n",
      "[168/201] Train Loss: 15082.8146 | Val Loss: 21774.4339\n",
      "[169/201] Train Loss: 15082.9517 | Val Loss: 21783.6468\n",
      "[170/201] Train Loss: 15082.2899 | Val Loss: 21780.6833\n",
      "[171/201] Train Loss: 15083.0735 | Val Loss: 21781.7863\n",
      "[172/201] Train Loss: 15082.9231 | Val Loss: 21789.9862\n",
      "[173/201] Train Loss: 15083.4144 | Val Loss: 21789.0912\n",
      "[174/201] Train Loss: 15082.4703 | Val Loss: 21787.3526\n",
      "[175/201] Train Loss: 15081.3849 | Val Loss: 21777.8990\n",
      "[176/201] Train Loss: 15081.9419 | Val Loss: 21784.6242\n",
      "[177/201] Train Loss: 15082.5165 | Val Loss: 21770.1341\n",
      "[178/201] Train Loss: 15082.6963 | Val Loss: 21778.3052\n",
      "[179/201] Train Loss: 15082.2807 | Val Loss: 21778.0930\n",
      "[180/201] Train Loss: 15082.3753 | Val Loss: 21774.8970\n",
      "[181/201] Train Loss: 15082.8009 | Val Loss: 21779.2199\n",
      "[182/201] Train Loss: 15081.3734 | Val Loss: 21781.3380\n",
      "[183/201] Train Loss: 15083.1650 | Val Loss: 21781.1483\n",
      "[184/201] Train Loss: 15082.2040 | Val Loss: 21788.6507\n",
      "[185/201] Train Loss: 15082.3815 | Val Loss: 21781.2669\n",
      "[186/201] Train Loss: 15083.1632 | Val Loss: 21783.7914\n",
      "[187/201] Train Loss: 15082.1702 | Val Loss: 21786.2002\n",
      "[188/201] Train Loss: 15081.6613 | Val Loss: 21786.2347\n",
      "[189/201] Train Loss: 15083.8617 | Val Loss: 21789.6940\n",
      "[190/201] Train Loss: 15080.7596 | Val Loss: 21786.2187\n",
      "[191/201] Train Loss: 15082.0413 | Val Loss: 21789.4857\n",
      "[192/201] Train Loss: 15082.7422 | Val Loss: 21791.9648\n",
      "[193/201] Train Loss: 15082.5991 | Val Loss: 21773.3622\n",
      "[194/201] Train Loss: 15081.3813 | Val Loss: 21777.5459\n",
      "[195/201] Train Loss: 15081.2508 | Val Loss: 21791.2934\n",
      "[196/201] Train Loss: 15082.2473 | Val Loss: 21792.6656\n",
      "[197/201] Train Loss: 15081.1812 | Val Loss: 21786.4865\n",
      "[198/201] Train Loss: 15082.5591 | Val Loss: 21775.8507\n",
      "[199/201] Train Loss: 15083.0439 | Val Loss: 21779.6712\n",
      "[200/201] Train Loss: 15081.9462 | Val Loss: 21781.1220\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\"C2D_LSTM_models/AE_LSTM_UCSD2.tar.pth\", train_loader, val_loader, learning_rate=1e-4, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:03,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "ucsd_test = UCSD(2, isTrain = False, image_size = 128, sample_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_prediction import FrameFeaturePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FrameFeaturePredictor(256, 256, isTrain = False, useGPU=False)\n",
    "load_model(test_model, \"C2D_LSTM_models/AE_LSTM_UCSD2.tar.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del tester\n",
    "except: pass\n",
    "tester = FramePredictor_Trainer(isTrain = False, useGPU = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_conv_features_lstm(self,\n",
    "                            model,\n",
    "                            feat_ext,\n",
    "                            test_data,\n",
    "                            batch_size = 8,\n",
    "                            stackFrames = 16,\n",
    "                            input_steps = 8,\n",
    "                            save_as = False):\n",
    "    model.to(self.device)\n",
    "    overall_targets, overall_losses = list(), list()\n",
    "    overall_roc_auc, overall_regularity_scores = list(), list()\n",
    "    features = list()\n",
    "    for directory_inputs, directory_labels in tqdm(test_data):\n",
    "        directory_targets, directory_loss = list(), list()\n",
    "\n",
    "        directory_input_features = list()\n",
    "        for idx in range(0, len(directory_inputs), batch_size):\n",
    "            extracted_features = feat_ext.extract_features(torch.stack(directory_inputs[idx: (idx + batch_size)]))\n",
    "            directory_input_features += extracted_features\n",
    "        directory_input_features = torch.stack(directory_input_features)\n",
    "        \n",
    "        for start_idx in range(0, (len(directory_input_features)//stackFrames)*stackFrames, stackFrames):\n",
    "            test_inputs = directory_input_features[start_idx : (start_idx + stackFrames)] # 16, 1, 128, 128\n",
    "            test_labels = directory_labels[start_idx : (start_idx + stackFrames)]\n",
    "            test_inputs = test_inputs.unsqueeze(dim = 1).to(self.device)\n",
    "            outputs = model.unroll(test_inputs[:input_steps], future_steps = (stackFrames - input_steps))\n",
    "            loss = self.loss_criterion(test_inputs[1:], outputs[:-1])\n",
    "\n",
    "            directory_loss += loss\n",
    "            directory_targets += test_labels[1:]\n",
    "        \n",
    "        regularity_scores = loss_to_regularity(directory_loss)\n",
    "        try:\n",
    "            directory_roc_auc = roc_auc_score(directory_targets, regularity_scores)\n",
    "        except:\n",
    "            directory_roc_auc = 1.0\n",
    "        overall_roc_auc.append(directory_roc_auc)\n",
    "        overall_regularity_scores.append(regularity_scores)\n",
    "\n",
    "        overall_targets.append(directory_targets)\n",
    "        overall_losses.append(directory_loss)\n",
    "#             overall_encodings.append(directory_encodings)\n",
    "    overall_targets = np.array(overall_targets)\n",
    "    overall_losses = np.array(overall_losses)\n",
    "#     overall_encodings = np.array(overall_encodings)\n",
    "\n",
    "    mean_roc_auc = np.mean(overall_roc_auc)\n",
    "\n",
    "    self.results = {\n",
    "        \"targets\": overall_targets,\n",
    "        \"losses\": overall_losses,\n",
    "        \"regularity\": overall_regularity_scores,\n",
    "        \"AUC_ROC_score\": overall_roc_auc,\n",
    "        \"final_AUC_ROC\":mean_roc_auc,\n",
    "    }\n",
    "\n",
    "    if save_as:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            pkl.dump(self.results, f)\n",
    "\n",
    "    return mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv_features_lstm(self,\n",
    "                            model,\n",
    "                            feat_ext,\n",
    "                            test_data,\n",
    "                            batch_size = 8,\n",
    "                            stackFrames = 16,\n",
    "                            input_steps = 8,\n",
    "                            save_as = False):\n",
    "    model.to(self.device)\n",
    "    overall_targets, overall_losses = list(), list()\n",
    "    overall_roc_auc, overall_regularity_scores = list(), list()\n",
    "    features = list()\n",
    "    for directory_inputs, directory_labels in tqdm(test_data):\n",
    "        directory_targets, directory_loss = list(), list()\n",
    "\n",
    "        directory_input_features = list()\n",
    "        for idx in range(0, len(directory_inputs), batch_size):\n",
    "            extracted_features = extract_features(torch.stack(directory_inputs[idx: (idx + batch_size)]))\n",
    "            directory_input_features += extracted_features\n",
    "        directory_input_features = torch.stack(directory_input_features)\n",
    "\n",
    "        for start_idx in range(0, (len(directory_input_features)//stackFrames)*stackFrames, stackFrames):\n",
    "            test_inputs = directory_input_features[start_idx : (start_idx + stackFrames)] # 16, 1, 128, 128\n",
    "            test_labels = directory_labels[start_idx : (start_idx + stackFrames)]\n",
    "            test_inputs = test_inputs.unsqueeze(dim = 1).to(self.device)\n",
    "            outputs = model.unroll(test_inputs[:input_steps], future_steps = (stackFrames - input_steps))\n",
    "            loss = self.loss_criterion(test_inputs[1:], outputs[:-1])\n",
    "\n",
    "            directory_loss += loss\n",
    "            directory_targets += test_labels[1:]\n",
    "\n",
    "        regularity_scores = loss_to_regularity(directory_loss)\n",
    "        try:\n",
    "            directory_roc_auc = roc_auc_score(directory_targets, regularity_scores)\n",
    "        except:\n",
    "            directory_roc_auc = 1.0\n",
    "        overall_roc_auc.append(directory_roc_auc)\n",
    "        overall_regularity_scores.append(regularity_scores)\n",
    "\n",
    "        overall_targets.append(directory_targets)\n",
    "        overall_losses.append(directory_loss)\n",
    "#             overall_encodings.append(directory_encodings)\n",
    "    overall_targets = np.array(overall_targets)\n",
    "    overall_losses = np.array(overall_losses)\n",
    "#     overall_encodings = np.array(overall_encodings)\n",
    "\n",
    "    mean_roc_auc = np.mean(overall_roc_auc)\n",
    "\n",
    "    self.results = {\n",
    "        \"targets\": overall_targets,\n",
    "        \"losses\": overall_losses,\n",
    "        \"regularity\": overall_regularity_scores,\n",
    "        \"AUC_ROC_score\": overall_roc_auc,\n",
    "        \"final_AUC_ROC\":mean_roc_auc,\n",
    "    }\n",
    "\n",
    "    if save_as:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            pkl.dump(self.results, f)\n",
    "\n",
    "    return mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:13<00:00,  1.14s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7651605775921029"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_features_lstm(\n",
    "            tester,\n",
    "            test_model,\n",
    "            feat_ext,\n",
    "            ucsd_test,\n",
    "            batch_size = 4,\n",
    "            stackFrames = 16,\n",
    "            input_steps = 8,\n",
    "            save_as = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing done\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
