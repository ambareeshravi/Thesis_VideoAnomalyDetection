{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_feature_extraction import ImageFeatureExtractor\n",
    "from train_frame_prediction import FramePredictor_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext = ImageFeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:59,  3.73s/it]\n"
     ]
    }
   ],
   "source": [
    "ucsd_train = UCSD(2, asImages = False, image_size=224, n_frames=16, sample_stride = 2)\n",
    "ucsd_processed_train = [(feat_ext.extract_features(data.transpose(0,1)), label) for (data, label) in ucsd_train]\n",
    "train_loader, val_loader = get_data_loaders(ucsd_processed_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del trainer\n",
    "except: pass\n",
    "trainer = FramePredictor_Trainer(4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/201] Train Loss: 198651.9214 | Val Loss: 141483.5174\n",
      "[2/201] Train Loss: 102704.8755 | Val Loss: 73971.7031\n",
      "[3/201] Train Loss: 56391.6702 | Val Loss: 48775.5634\n",
      "[4/201] Train Loss: 38590.3830 | Val Loss: 41108.6987\n",
      "[5/201] Train Loss: 31536.3613 | Val Loss: 37381.1231\n",
      "[6/201] Train Loss: 28545.3568 | Val Loss: 36618.4157\n",
      "[7/201] Train Loss: 27084.9239 | Val Loss: 34959.1248\n",
      "[8/201] Train Loss: 26145.9647 | Val Loss: 35175.0307\n",
      "[9/201] Train Loss: 25371.8569 | Val Loss: 35460.1575\n",
      "[10/201] Train Loss: 24789.9052 | Val Loss: 35569.1887\n",
      "[11/201] Train Loss: 24119.9921 | Val Loss: 34139.6908\n",
      "[12/201] Train Loss: 23581.8394 | Val Loss: 37743.8377\n",
      "[13/201] Train Loss: 23150.0095 | Val Loss: 35103.0401\n",
      "[14/201] Train Loss: 22628.8394 | Val Loss: 34654.9622\n",
      "[15/201] Train Loss: 22209.3361 | Val Loss: 33939.2583\n",
      "[16/201] Train Loss: 21810.3666 | Val Loss: 33980.9229\n",
      "[17/201] Train Loss: 21447.2335 | Val Loss: 33695.0071\n",
      "[18/201] Train Loss: 21082.6865 | Val Loss: 31838.9600\n",
      "[19/201] Train Loss: 20746.0699 | Val Loss: 35586.8977\n",
      "[20/201] Train Loss: 20441.0437 | Val Loss: 35864.6309\n",
      "[21/201] Train Loss: 20171.6802 | Val Loss: 33508.2663\n",
      "[22/201] Train Loss: 19863.6291 | Val Loss: 31855.4776\n",
      "[23/201] Train Loss: 19557.8596 | Val Loss: 34015.3420\n",
      "[24/201] Train Loss: 19228.6625 | Val Loss: 29448.7951\n",
      "[25/201] Train Loss: 18926.2431 | Val Loss: 29500.0650\n",
      "[26/201] Train Loss: 18765.7811 | Val Loss: 29511.7062\n",
      "[27/201] Train Loss: 18582.5901 | Val Loss: 30920.0295\n",
      "[28/201] Train Loss: 18422.1837 | Val Loss: 29683.4583\n",
      "[29/201] Train Loss: 18209.7653 | Val Loss: 30848.0214\n",
      "[30/201] Train Loss: 18020.8123 | Val Loss: 27776.5714\n",
      "[31/201] Train Loss: 17849.4505 | Val Loss: 28114.4634\n",
      "[32/201] Train Loss: 17726.8910 | Val Loss: 30112.1092\n",
      "[33/201] Train Loss: 17590.0377 | Val Loss: 28799.8620\n",
      "[34/201] Train Loss: 17493.1466 | Val Loss: 27354.3258\n",
      "[35/201] Train Loss: 17368.3800 | Val Loss: 28851.0609\n",
      "[36/201] Train Loss: 17246.7962 | Val Loss: 27558.2701\n",
      "[37/201] Train Loss: 17151.8570 | Val Loss: 27301.8000\n",
      "[38/201] Train Loss: 17005.2919 | Val Loss: 28474.9990\n",
      "[39/201] Train Loss: 16898.0842 | Val Loss: 28089.2953\n",
      "[40/201] Train Loss: 16823.5881 | Val Loss: 30453.1003\n",
      "[41/201] Train Loss: 16706.8508 | Val Loss: 27754.0044\n",
      "[42/201] Train Loss: 16587.9786 | Val Loss: 28629.5851\n",
      "[43/201] Train Loss: 16425.5172 | Val Loss: 28108.7924\n",
      "[44/201] Train Loss: 16326.4697 | Val Loss: 26674.2789\n",
      "[45/201] Train Loss: 16241.1120 | Val Loss: 26027.0589\n",
      "[46/201] Train Loss: 16194.0062 | Val Loss: 27602.4662\n",
      "[47/201] Train Loss: 16109.5091 | Val Loss: 27189.4046\n",
      "[48/201] Train Loss: 16053.0177 | Val Loss: 26721.0347\n",
      "[49/201] Train Loss: 15969.4877 | Val Loss: 29414.7892\n",
      "[50/201] Train Loss: 15893.9401 | Val Loss: 28808.8578\n",
      "[51/201] Train Loss: 15789.4447 | Val Loss: 26323.8487\n",
      "[52/201] Train Loss: 15708.5364 | Val Loss: 25889.2852\n",
      "[53/201] Train Loss: 15672.6896 | Val Loss: 25876.8448\n",
      "[54/201] Train Loss: 15601.4558 | Val Loss: 26593.3852\n",
      "[55/201] Train Loss: 15551.5396 | Val Loss: 25920.7420\n",
      "[56/201] Train Loss: 15504.9809 | Val Loss: 26145.7026\n",
      "[57/201] Train Loss: 15443.0250 | Val Loss: 25989.6039\n",
      "[58/201] Train Loss: 15393.7395 | Val Loss: 25946.7019\n",
      "[59/201] Train Loss: 15310.7436 | Val Loss: 25689.9289\n",
      "[60/201] Train Loss: 15271.5427 | Val Loss: 25770.9783\n",
      "[61/201] Train Loss: 15236.9071 | Val Loss: 25409.5964\n",
      "[62/201] Train Loss: 15192.7599 | Val Loss: 25112.2431\n",
      "[63/201] Train Loss: 15139.6338 | Val Loss: 24966.0459\n",
      "[64/201] Train Loss: 15100.1927 | Val Loss: 25353.3145\n",
      "[65/201] Train Loss: 15060.8869 | Val Loss: 25091.5748\n",
      "[66/201] Train Loss: 15023.4010 | Val Loss: 25668.9866\n",
      "[67/201] Train Loss: 14996.3537 | Val Loss: 25251.0162\n",
      "[68/201] Train Loss: 14953.2786 | Val Loss: 26147.7301\n",
      "[69/201] Train Loss: 14901.6503 | Val Loss: 25106.6974\n",
      "[70/201] Train Loss: 14854.8848 | Val Loss: 25968.8679\n",
      "[71/201] Train Loss: 14835.3555 | Val Loss: 25177.5406\n",
      "[72/201] Train Loss: 14795.0792 | Val Loss: 25279.8679\n",
      "[73/201] Train Loss: 14768.9344 | Val Loss: 25297.5208\n",
      "[74/201] Train Loss: 14739.1948 | Val Loss: 24725.6657\n",
      "[75/201] Train Loss: 14702.5882 | Val Loss: 24517.4032\n",
      "[76/201] Train Loss: 14679.3980 | Val Loss: 24900.9572\n",
      "[77/201] Train Loss: 14654.7944 | Val Loss: 24461.0775\n",
      "[78/201] Train Loss: 14645.2025 | Val Loss: 24121.8628\n",
      "[79/201] Train Loss: 14609.8944 | Val Loss: 24950.5137\n",
      "[80/201] Train Loss: 14591.0455 | Val Loss: 24981.0154\n",
      "[81/201] Train Loss: 14570.5764 | Val Loss: 24449.3753\n",
      "[82/201] Train Loss: 14551.1916 | Val Loss: 24424.4942\n",
      "[83/201] Train Loss: 14533.0162 | Val Loss: 24559.3854\n",
      "[84/201] Train Loss: 14490.6389 | Val Loss: 24451.0850\n",
      "[85/201] Train Loss: 14480.4687 | Val Loss: 24066.9267\n",
      "[86/201] Train Loss: 14458.4167 | Val Loss: 24312.7862\n",
      "[87/201] Train Loss: 14443.1146 | Val Loss: 24222.5874\n",
      "[88/201] Train Loss: 14427.4248 | Val Loss: 24749.8404\n",
      "[89/201] Train Loss: 14408.4257 | Val Loss: 24248.5726\n",
      "[90/201] Train Loss: 14389.2944 | Val Loss: 24212.1130\n",
      "[91/201] Train Loss: 14375.2749 | Val Loss: 24225.0552\n",
      "[92/201] Train Loss: 14351.7229 | Val Loss: 24358.1847\n",
      "[93/201] Train Loss: 14336.9203 | Val Loss: 24601.1236\n",
      "[94/201] Train Loss: 14325.9203 | Val Loss: 24507.4292\n",
      "[95/201] Train Loss: 14320.0231 | Val Loss: 24287.2804\n",
      "[96/201] Train Loss: 14291.6375 | Val Loss: 24166.9419\n",
      "[97/201] Train Loss: 14280.8963 | Val Loss: 24156.6065\n",
      "[98/201] Train Loss: 14274.0588 | Val Loss: 24077.1859\n",
      "[99/201] Train Loss: 14258.0127 | Val Loss: 24135.0332\n",
      "[100/201] Train Loss: 14247.0260 | Val Loss: 24295.5350\n",
      "[101/201] Train Loss: 14241.8965 | Val Loss: 24184.9588\n",
      "[102/201] Train Loss: 14232.1873 | Val Loss: 24131.3468\n",
      "[103/201] Train Loss: 14228.4767 | Val Loss: 24130.4962\n",
      "[104/201] Train Loss: 14227.3656 | Val Loss: 24505.6236\n",
      "[105/201] Train Loss: 14215.2041 | Val Loss: 24200.5178\n",
      "[106/201] Train Loss: 14197.2071 | Val Loss: 24194.0632\n",
      "[107/201] Train Loss: 14193.8035 | Val Loss: 24113.8212\n",
      "[108/201] Train Loss: 14183.5494 | Val Loss: 24240.6154\n",
      "[109/201] Train Loss: 14180.0177 | Val Loss: 24152.7524\n",
      "[110/201] Train Loss: 14177.0877 | Val Loss: 24101.7459\n",
      "[111/201] Train Loss: 14175.2527 | Val Loss: 23979.6599\n",
      "[112/201] Train Loss: 14161.5956 | Val Loss: 24094.2346\n",
      "[113/201] Train Loss: 14160.7396 | Val Loss: 23972.3524\n",
      "[114/201] Train Loss: 14160.0186 | Val Loss: 23919.2457\n",
      "[115/201] Train Loss: 14148.7561 | Val Loss: 24017.2610\n",
      "[116/201] Train Loss: 14153.5174 | Val Loss: 23985.7603\n",
      "[117/201] Train Loss: 14148.0439 | Val Loss: 23859.9571\n",
      "[118/201] Train Loss: 14134.8155 | Val Loss: 24017.7308\n",
      "[119/201] Train Loss: 14137.1248 | Val Loss: 24086.3088\n",
      "[120/201] Train Loss: 14132.6171 | Val Loss: 24145.6206\n",
      "[121/201] Train Loss: 14124.1563 | Val Loss: 23888.3934\n",
      "[122/201] Train Loss: 14128.1111 | Val Loss: 24118.8565\n",
      "[123/201] Train Loss: 14111.8527 | Val Loss: 23832.8567\n",
      "[124/201] Train Loss: 14109.5064 | Val Loss: 24124.5681\n",
      "[125/201] Train Loss: 14113.6954 | Val Loss: 23912.2055\n",
      "[126/201] Train Loss: 14113.5113 | Val Loss: 23908.8124\n",
      "[127/201] Train Loss: 14103.2044 | Val Loss: 23951.9320\n",
      "[128/201] Train Loss: 14102.9143 | Val Loss: 24014.5976\n",
      "[129/201] Train Loss: 14089.7491 | Val Loss: 23918.2048\n",
      "[130/201] Train Loss: 14098.5135 | Val Loss: 23843.3700\n",
      "[131/201] Train Loss: 14090.5534 | Val Loss: 23897.2886\n",
      "[132/201] Train Loss: 14087.2806 | Val Loss: 23853.2225\n",
      "[133/201] Train Loss: 14080.7859 | Val Loss: 24037.9078\n",
      "[134/201] Train Loss: 14082.5420 | Val Loss: 23907.2914\n",
      "[135/201] Train Loss: 14078.6487 | Val Loss: 24030.5742\n",
      "[136/201] Train Loss: 14071.4733 | Val Loss: 23913.5899\n",
      "[137/201] Train Loss: 14078.6907 | Val Loss: 23824.8526\n",
      "[138/201] Train Loss: 14072.0999 | Val Loss: 23982.2113\n",
      "[139/201] Train Loss: 14070.2011 | Val Loss: 23855.5757\n",
      "[140/201] Train Loss: 14067.8842 | Val Loss: 23806.9381\n",
      "[141/201] Train Loss: 14065.0104 | Val Loss: 23827.2337\n",
      "[142/201] Train Loss: 14070.5028 | Val Loss: 23891.2284\n",
      "[143/201] Train Loss: 14061.9700 | Val Loss: 23858.5165\n",
      "[144/201] Train Loss: 14056.4297 | Val Loss: 23869.2676\n",
      "[145/201] Train Loss: 14066.9613 | Val Loss: 23873.3930\n",
      "[146/201] Train Loss: 14056.9571 | Val Loss: 23881.5556\n",
      "[147/201] Train Loss: 14051.7983 | Val Loss: 23856.7296\n",
      "[148/201] Train Loss: 14056.1279 | Val Loss: 23930.2543\n",
      "[149/201] Train Loss: 14053.8668 | Val Loss: 23865.6015\n",
      "[150/201] Train Loss: 14050.2995 | Val Loss: 23946.8347\n",
      "[151/201] Train Loss: 14049.5075 | Val Loss: 23798.1316\n",
      "[152/201] Train Loss: 14049.9536 | Val Loss: 23884.6043\n",
      "[153/201] Train Loss: 14042.6630 | Val Loss: 23871.7420\n",
      "[154/201] Train Loss: 14048.7127 | Val Loss: 23841.6841\n",
      "[155/201] Train Loss: 14050.4247 | Val Loss: 23871.7210\n",
      "[156/201] Train Loss: 14041.4293 | Val Loss: 23877.8448\n",
      "[157/201] Train Loss: 14043.7170 | Val Loss: 23848.0888\n",
      "[158/201] Train Loss: 14040.2995 | Val Loss: 23854.7043\n",
      "[159/201] Train Loss: 14043.5008 | Val Loss: 23857.7536\n",
      "[160/201] Train Loss: 14034.2614 | Val Loss: 23853.4219\n",
      "[161/201] Train Loss: 14039.2701 | Val Loss: 23894.0500\n",
      "[162/201] Train Loss: 14034.7341 | Val Loss: 23868.6955\n",
      "[163/201] Train Loss: 14034.4940 | Val Loss: 23927.7372\n",
      "[164/201] Train Loss: 14034.3222 | Val Loss: 23871.5644\n",
      "[165/201] Train Loss: 14037.1047 | Val Loss: 23842.8540\n",
      "[166/201] Train Loss: 14031.8539 | Val Loss: 23893.5802\n",
      "[167/201] Train Loss: 14034.4586 | Val Loss: 23827.6483\n",
      "[168/201] Train Loss: 14031.1695 | Val Loss: 23877.5174\n",
      "[169/201] Train Loss: 14025.3234 | Val Loss: 23812.4241\n",
      "[170/201] Train Loss: 14031.9817 | Val Loss: 23874.7931\n",
      "[171/201] Train Loss: 14032.6621 | Val Loss: 23872.7137\n",
      "[172/201] Train Loss: 14025.4531 | Val Loss: 23869.9889\n",
      "[173/201] Train Loss: 14029.5988 | Val Loss: 23900.2418\n",
      "[174/201] Train Loss: 14025.3896 | Val Loss: 23880.3603\n",
      "[175/201] Train Loss: 14030.1930 | Val Loss: 23845.6582\n",
      "[176/201] Train Loss: 14030.1874 | Val Loss: 23866.3295\n",
      "[177/201] Train Loss: 14028.2941 | Val Loss: 23881.3771\n",
      "[178/201] Train Loss: 14022.5263 | Val Loss: 23860.6384\n",
      "[179/201] Train Loss: 14022.0884 | Val Loss: 23841.7085\n",
      "[180/201] Train Loss: 14033.6323 | Val Loss: 23877.8572\n",
      "[181/201] Train Loss: 14022.0219 | Val Loss: 23841.6229\n",
      "[182/201] Train Loss: 14029.8158 | Val Loss: 23837.6412\n",
      "[183/201] Train Loss: 14025.9168 | Val Loss: 23831.7017\n",
      "[184/201] Train Loss: 14026.3220 | Val Loss: 23862.2581\n",
      "[185/201] Train Loss: 14024.0965 | Val Loss: 23871.0669\n",
      "[186/201] Train Loss: 14027.9717 | Val Loss: 23889.8883\n",
      "[187/201] Train Loss: 14026.4570 | Val Loss: 23868.6250\n",
      "[188/201] Train Loss: 14033.3544 | Val Loss: 23872.3462\n",
      "[189/201] Train Loss: 14023.2679 | Val Loss: 23852.5384\n",
      "[190/201] Train Loss: 14027.3333 | Val Loss: 23860.7413\n",
      "[191/201] Train Loss: 14020.4290 | Val Loss: 23877.9093\n",
      "[192/201] Train Loss: 14024.0267 | Val Loss: 23853.9370\n",
      "[193/201] Train Loss: 14021.8681 | Val Loss: 23868.4498\n",
      "[194/201] Train Loss: 14025.1858 | Val Loss: 23863.0686\n",
      "[195/201] Train Loss: 14025.8236 | Val Loss: 23881.4422\n",
      "[196/201] Train Loss: 14019.9022 | Val Loss: 23873.4165\n",
      "[197/201] Train Loss: 14018.2727 | Val Loss: 23852.1824\n",
      "[198/201] Train Loss: 14019.0955 | Val Loss: 23873.9689\n",
      "[199/201] Train Loss: 14019.8481 | Val Loss: 23879.8654\n",
      "[200/201] Train Loss: 14030.7780 | Val Loss: 23877.7109\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\"C2D_LSTM_models/C2D_LSTM_UCSD2.tar.pth\", train_loader, val_loader, learning_rate=1e-4, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:05,  2.35it/s]\n"
     ]
    }
   ],
   "source": [
    "ucsd_test = UCSD(2, isTrain = False, image_size = 224, sample_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_prediction import FrameFeaturePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FrameFeaturePredictor(4096, 4096, isTrain = False)\n",
    "load_model(test_model, \"C2D_LSTM_models/C2D_LSTM_UCSD2.tar.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del tester\n",
    "except: pass\n",
    "tester = FramePredictor_Trainer(isTrain = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_conv_features_lstm(self,\n",
    "                            model,\n",
    "                            feat_ext,\n",
    "                            test_data,\n",
    "                            batch_size = 8,\n",
    "                            stackFrames = 16,\n",
    "                            input_steps = 8,\n",
    "                            save_as = False):\n",
    "    model.to(self.device)\n",
    "    overall_targets, overall_losses = list(), list()\n",
    "    overall_roc_auc, overall_regularity_scores = list(), list()\n",
    "    features = list()\n",
    "    for directory_inputs, directory_labels in tqdm(test_data):\n",
    "        directory_targets, directory_loss = list(), list()\n",
    "\n",
    "        directory_input_features = list()\n",
    "        for idx in range(0, len(directory_inputs), batch_size):\n",
    "            extracted_features = feat_ext.extract_features(torch.stack(directory_inputs[idx: (idx + batch_size)]))\n",
    "            directory_input_features += extracted_features\n",
    "        directory_input_features = torch.stack(directory_input_features)\n",
    "        \n",
    "        for start_idx in range(0, (len(directory_input_features)//stackFrames)*stackFrames, stackFrames):\n",
    "            test_inputs = directory_input_features[start_idx : (start_idx + stackFrames)] # 16, 1, 128, 128\n",
    "            test_labels = directory_labels[start_idx : (start_idx + stackFrames)]\n",
    "            test_inputs = test_inputs.unsqueeze(dim = 1).to(self.device)\n",
    "            outputs = model.unroll(test_inputs[:input_steps], future_steps = (stackFrames - input_steps))\n",
    "            loss = self.loss_criterion(test_inputs[1:], outputs[:-1])\n",
    "\n",
    "            directory_loss += loss\n",
    "            directory_targets += test_labels[1:]\n",
    "        \n",
    "        regularity_scores = loss_to_regularity(directory_loss)\n",
    "        try:\n",
    "            directory_roc_auc = roc_auc_score(directory_targets, regularity_scores)\n",
    "        except:\n",
    "            directory_roc_auc = 1.0\n",
    "        overall_roc_auc.append(directory_roc_auc)\n",
    "        overall_regularity_scores.append(regularity_scores)\n",
    "\n",
    "        overall_targets.append(directory_targets)\n",
    "        overall_losses.append(directory_loss)\n",
    "#             overall_encodings.append(directory_encodings)\n",
    "    overall_targets = np.array(overall_targets)\n",
    "    overall_losses = np.array(overall_losses)\n",
    "#     overall_encodings = np.array(overall_encodings)\n",
    "\n",
    "    mean_roc_auc = np.mean(overall_roc_auc)\n",
    "\n",
    "    self.results = {\n",
    "        \"targets\": overall_targets,\n",
    "        \"losses\": overall_losses,\n",
    "        \"regularity\": overall_regularity_scores,\n",
    "        \"AUC_ROC_score\": overall_roc_auc,\n",
    "        \"final_AUC_ROC\":mean_roc_auc,\n",
    "    }\n",
    "\n",
    "    if save_as:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            pkl.dump(self.results, f)\n",
    "\n",
    "    return mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7765722486621952"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_features_lstm(\n",
    "            tester,\n",
    "            test_model,\n",
    "            feat_ext,\n",
    "            ucsd_test,\n",
    "            batch_size = 4,\n",
    "            stackFrames = 16,\n",
    "            input_steps = 8,\n",
    "            save_as = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing done\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_ext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-639533f56b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mfeat_ext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_ext' is not defined"
     ]
    }
   ],
   "source": [
    "del feat_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
