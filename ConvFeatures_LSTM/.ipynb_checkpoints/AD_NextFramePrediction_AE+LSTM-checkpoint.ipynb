{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_feature_extraction import ImageFeatureExtractor\n",
    "from train_frame_prediction import FramePredictor_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext = ImageFeatureExtractor(useGPU = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoders.C2D_Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C2D_AE_128_3x3(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): ConvTranspose2d(64, 1, kernel_size=(3, 3), stride=(2, 2), output_padding=(1, 1))\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = C2D_AE_128_3x3(channels=1)\n",
    "load_model(model, \"../AutoEncoders/C2D_AE_models/C2D_AE_128_3x3_UCSD2/C2D_AE_128_3x3_UCSD2.pth.tar\")\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_ext.feature_extractor = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        return feat_ext.feature_extractor(images.to(feat_ext.device))[-1].detach().cpu().flatten(start_dim = 1, end_dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:47,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "ucsd_train = UCSD(2, asImages = False, image_size=128, n_frames=16, sample_stride = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucsd_processed_train = [(extract_features(data.transpose(0,1)), label) for (data, label) in ucsd_train]\n",
    "train_loader, val_loader = get_data_loaders(ucsd_processed_train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del trainer\n",
    "except: pass\n",
    "trainer = FramePredictor_Trainer(256, 256, useGPU=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_prediction import FrameFeaturePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = FrameFeaturePredictor(256,256, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\"C2D_LSTM_models/AE_LSTM_UCSD2.tar.pth\", train_loader, val_loader, learning_rate=1e-4, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucsd_test = UCSD(2, isTrain = False, image_size = 128, sample_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from frame_prediction import FrameFeaturePredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = FrameFeaturePredictor(256, 256, isTrain = False, useGPU=False)\n",
    "load_model(test_model, \"C2D_LSTM_models/AE_LSTM_UCSD2.tar.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del tester\n",
    "except: pass\n",
    "tester = FramePredictor_Trainer(isTrain = False, useGPU = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_conv_features_lstm(self,\n",
    "                            model,\n",
    "                            feat_ext,\n",
    "                            test_data,\n",
    "                            batch_size = 8,\n",
    "                            stackFrames = 16,\n",
    "                            input_steps = 8,\n",
    "                            save_as = False):\n",
    "    model.to(self.device)\n",
    "    overall_targets, overall_losses = list(), list()\n",
    "    overall_roc_auc, overall_regularity_scores = list(), list()\n",
    "    features = list()\n",
    "    for directory_inputs, directory_labels in tqdm(test_data):\n",
    "        directory_targets, directory_loss = list(), list()\n",
    "\n",
    "        directory_input_features = list()\n",
    "        for idx in range(0, len(directory_inputs), batch_size):\n",
    "            extracted_features = feat_ext.extract_features(torch.stack(directory_inputs[idx: (idx + batch_size)]))\n",
    "            directory_input_features += extracted_features\n",
    "        directory_input_features = torch.stack(directory_input_features)\n",
    "        \n",
    "        for start_idx in range(0, (len(directory_input_features)//stackFrames)*stackFrames, stackFrames):\n",
    "            test_inputs = directory_input_features[start_idx : (start_idx + stackFrames)] # 16, 1, 128, 128\n",
    "            test_labels = directory_labels[start_idx : (start_idx + stackFrames)]\n",
    "            test_inputs = test_inputs.unsqueeze(dim = 1).to(self.device)\n",
    "            outputs = model.unroll(test_inputs[:input_steps], future_steps = (stackFrames - input_steps))\n",
    "            loss = self.loss_criterion(test_inputs[1:], outputs[:-1])\n",
    "\n",
    "            directory_loss += loss\n",
    "            directory_targets += test_labels[1:]\n",
    "        \n",
    "        regularity_scores = loss_to_regularity(directory_loss)\n",
    "        try:\n",
    "            directory_roc_auc = roc_auc_score(directory_targets, regularity_scores)\n",
    "        except:\n",
    "            directory_roc_auc = 1.0\n",
    "        overall_roc_auc.append(directory_roc_auc)\n",
    "        overall_regularity_scores.append(regularity_scores)\n",
    "\n",
    "        overall_targets.append(directory_targets)\n",
    "        overall_losses.append(directory_loss)\n",
    "#             overall_encodings.append(directory_encodings)\n",
    "    overall_targets = np.array(overall_targets)\n",
    "    overall_losses = np.array(overall_losses)\n",
    "#     overall_encodings = np.array(overall_encodings)\n",
    "\n",
    "    mean_roc_auc = np.mean(overall_roc_auc)\n",
    "\n",
    "    self.results = {\n",
    "        \"targets\": overall_targets,\n",
    "        \"losses\": overall_losses,\n",
    "        \"regularity\": overall_regularity_scores,\n",
    "        \"AUC_ROC_score\": overall_roc_auc,\n",
    "        \"final_AUC_ROC\":mean_roc_auc,\n",
    "    }\n",
    "\n",
    "    if save_as:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            pkl.dump(self.results, f)\n",
    "\n",
    "    return mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conv_features_lstm(self,\n",
    "                            model,\n",
    "                            feat_ext,\n",
    "                            test_data,\n",
    "                            batch_size = 8,\n",
    "                            stackFrames = 16,\n",
    "                            input_steps = 8,\n",
    "                            save_as = False):\n",
    "    model.to(self.device)\n",
    "    overall_targets, overall_losses = list(), list()\n",
    "    overall_roc_auc, overall_regularity_scores = list(), list()\n",
    "    features = list()\n",
    "    for directory_inputs, directory_labels in tqdm(test_data):\n",
    "        directory_targets, directory_loss = list(), list()\n",
    "\n",
    "        directory_input_features = list()\n",
    "        for idx in range(0, len(directory_inputs), batch_size):\n",
    "            extracted_features = extract_features(torch.stack(directory_inputs[idx: (idx + batch_size)]))\n",
    "            directory_input_features += extracted_features\n",
    "        directory_input_features = torch.stack(directory_input_features)\n",
    "\n",
    "        for start_idx in range(0, (len(directory_input_features)//stackFrames)*stackFrames, stackFrames):\n",
    "            test_inputs = directory_input_features[start_idx : (start_idx + stackFrames)] # 16, 1, 128, 128\n",
    "            test_labels = directory_labels[start_idx : (start_idx + stackFrames)]\n",
    "            test_inputs = test_inputs.unsqueeze(dim = 1).to(self.device)\n",
    "            outputs = model.unroll(test_inputs[:input_steps], future_steps = (stackFrames - input_steps))\n",
    "            loss = self.loss_criterion(test_inputs[1:], outputs[:-1])\n",
    "\n",
    "            directory_loss += loss\n",
    "            directory_targets += test_labels[1:]\n",
    "\n",
    "        regularity_scores = loss_to_regularity(directory_loss)\n",
    "        try:\n",
    "            directory_roc_auc = roc_auc_score(directory_targets, regularity_scores)\n",
    "        except:\n",
    "            directory_roc_auc = 1.0\n",
    "        overall_roc_auc.append(directory_roc_auc)\n",
    "        overall_regularity_scores.append(regularity_scores)\n",
    "\n",
    "        overall_targets.append(directory_targets)\n",
    "        overall_losses.append(directory_loss)\n",
    "#             overall_encodings.append(directory_encodings)\n",
    "    overall_targets = np.array(overall_targets)\n",
    "    overall_losses = np.array(overall_losses)\n",
    "#     overall_encodings = np.array(overall_encodings)\n",
    "\n",
    "    mean_roc_auc = np.mean(overall_roc_auc)\n",
    "\n",
    "    self.results = {\n",
    "        \"targets\": overall_targets,\n",
    "        \"losses\": overall_losses,\n",
    "        \"regularity\": overall_regularity_scores,\n",
    "        \"AUC_ROC_score\": overall_roc_auc,\n",
    "        \"final_AUC_ROC\":mean_roc_auc,\n",
    "    }\n",
    "\n",
    "    if save_as:\n",
    "        with open(save_as, \"wb\") as f:\n",
    "            pkl.dump(self.results, f)\n",
    "\n",
    "    return mean_roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv_features_lstm(\n",
    "            tester,\n",
    "            test_model,\n",
    "            feat_ext,\n",
    "            ucsd_test,\n",
    "            batch_size = 4,\n",
    "            stackFrames = 16,\n",
    "            input_steps = 8,\n",
    "            save_as = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
