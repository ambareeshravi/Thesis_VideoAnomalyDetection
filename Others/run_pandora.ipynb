{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7ff9c02e0630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from general.data import *\n",
    "from AutoEncoders import train_autoencoder\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2D_AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer_info, decoder_layer_info):\n",
    "        super(C2D_AutoEncoder, self).__init__()\n",
    "        \n",
    "        encoder_layers = list()\n",
    "        for idx, (c_in, c_out, ck, cs, cp, cd, pk, ps) in enumerate(encoder_layer_info):\n",
    "            layers = [\n",
    "                nn.Conv2d(c_in, c_out, ck, cs, cp, cd),\n",
    "                nn.BatchNorm2d(c_out),\n",
    "            ]\n",
    "            if idx < (len(encoder_layer_info) - 1):\n",
    "                layers += [\n",
    "                    nn.LeakyReLU(0.02,),\n",
    "                    nn.MaxPool2d(pk, ps)\n",
    "                ]\n",
    "            else:\n",
    "                layers += [nn.Tanh()]\n",
    "            encoder_layers.append(nn.Sequential(*layers))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        decoder_layers = list()\n",
    "        for idx, (ct_in, ct_out, ctk, cts, ctp, cto, usf) in enumerate(decoder_layer_info):\n",
    "            layers = [\n",
    "                nn.ConvTranspose2d(ct_in, ct_out, ctk, cts, ctp, cto),\n",
    "                nn.BatchNorm2d(ct_out),\n",
    "            ]\n",
    "            if idx < (len(decoder_layer_info) - 1):\n",
    "                layers += [\n",
    "                    nn.LeakyReLU(0.02,),\n",
    "                    nn.Upsample(scale_factor=usf),\n",
    "                ]\n",
    "            else:\n",
    "                layers += [nn.Sigmoid()]\n",
    "            decoder_layers.append(nn.Sequential(*layers))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        encodings = self.encoder(x)\n",
    "        reconstructions = self.decoder(encodings)\n",
    "        return reconstructions, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandoraAE(nn.Module):\n",
    "    def __init__(self, channels = 1, isTrain = True, useGPU = True):\n",
    "        super(PandoraAE, self).__init__()\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if useGPU and torch.cuda.is_available(): self.device = torch.device(\"cuda:0\")\n",
    "        self.c2d_ae = list()\n",
    "        \n",
    "        eli1, dli1 = self.get_info(channels,64,64)\n",
    "        self.c2d_ae.append(C2D_AutoEncoder(eli1, dli1).to(self.device))\n",
    "        eli2, dli2 = self.get_info(64,64,128)\n",
    "        self.c2d_ae.append(C2D_AutoEncoder(eli2, dli2).to(self.device))\n",
    "#         eli3, dli3 = self.get_info(128,128,256)\n",
    "        self.c2d_ae.append(C2D_AutoEncoder(\n",
    "                                encoder_layer_info = [\n",
    "                                    [128,128,3,1,0,1,2,1],\n",
    "                                    [128,256,3,2,0,1,2,1],\n",
    "                                ],\n",
    "                                decoder_layer_info = [\n",
    "                                    [256,128,4,2,0,1,1], \n",
    "                                    [128,128,3,1,0,0,1],\n",
    "                                ]).to(self.device))\n",
    "        eli4, dli4 = self.get_info(256,128,64)\n",
    "        self.c2d_ae.append(C2D_AutoEncoder(eli4, dli4).to(self.device))\n",
    "        self.isTrain = isTrain\n",
    "                    \n",
    "    def get_info(self, n1, n2, n3):\n",
    "        encoder_layer_info = [\n",
    "            [n1,n2,3,1,0,1,2,1],\n",
    "            [n2,n3,3,2,0,1,2,1],\n",
    "        ]\n",
    "        decoder_layer_info = [\n",
    "            [n3,n2,3,2,0,1,1], \n",
    "            [n2,n1,3,1,0,0,1],\n",
    "        ]\n",
    "        return encoder_layer_info, decoder_layer_info\n",
    "    \n",
    "    def forward(self, input_images):\n",
    "        if self.isTrain:\n",
    "            c2d_ae1_reconstructions, c2d_ae1_encodings = self.c2d_ae[0](input_images)\n",
    "            c2d_ae2_reconstructions, c2d_ae2_encodings = self.c2d_ae[1](c2d_ae1_encodings.detach())\n",
    "            c2d_ae3_reconstructions, c2d_ae3_encodings = self.c2d_ae[2](c2d_ae2_encodings.detach())\n",
    "            c2d_ae4_reconstructions, c2d_ae4_encodings = self.c2d_ae[3](c2d_ae3_encodings.detach())\n",
    "            return [c2d_ae1_reconstructions, c2d_ae2_reconstructions, c2d_ae3_reconstructions, c2d_ae4_reconstructions], [c2d_ae1_encodings, c2d_ae2_encodings, c2d_ae3_encodings, c2d_ae4_encodings]\n",
    "        else:\n",
    "            c2d_a1_encodings = self.c2d_ae[0].encoder(input_images)\n",
    "            c2d_a2_encodings = self.c2d_ae[1].encoder(c2d_a1_encodings)\n",
    "            c2d_a3_encodings = self.c2d_ae[2].encoder(c2d_a2_encodings)\n",
    "            c2d_a4_encodings = self.c2d_ae[3].encoder(c2d_a3_encodings)\n",
    "\n",
    "            c2d_a4_reconstructions = self.c2d_ae[3].decoder(c2d_a4_encodings)\n",
    "            c2d_a3_reconstructions = self.c2d_ae[2].decoder(c2d_a3_encodings)\n",
    "            c2d_a2_reconstructions = self.c2d_ae[1].decoder(c2d_a2_encodings)\n",
    "            c2d_a1_reconstructions = self.c2d_ae[0].decoder(c2d_a1_encodings)\n",
    "\n",
    "            return c2d_a1_reconstructions, c2d_a4_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandoraModel:\n",
    "    def __init__(self,\n",
    "                 model_type = \"pandora_c2d_AE\",\n",
    "                 learning_rate=1e-3,\n",
    "                 LRDecay = 0.75,\n",
    "                 LRPatience = 4,\n",
    "                 LRThreshold = 5e-4,\n",
    "                 parent_directory = \"models/\",\n",
    "                 useGPU = True,\n",
    "                 debug = True):\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.debug = debug\n",
    "        if self.debug: print(\"%s READY\"%(model_type))\n",
    "        \n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if useGPU and torch.cuda.is_available(): self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        # Path\n",
    "        self.save_path = os.path.join(parent_directory, model_type)\n",
    "        if not os.path.exists(self.save_path): os.mkdir(self.save_path)\n",
    "        self.model_file = os.path.join(self.save_path, model_type + \".pth.tar\")\n",
    "        \n",
    "        # Model Params\n",
    "        self.stopTraining = False\n",
    "        self.history = {\n",
    "            \"train_loss\": list(),\n",
    "            \"validation_loss\": list()\n",
    "        }\n",
    "        \n",
    "        self.epoch_train_loss = list()\n",
    "        self.epoch_validation_loss = list()\n",
    "        \n",
    "        # Model Essentials\n",
    "        self.model = PandoraAE()\n",
    "        self.model.to(self.device)\n",
    "        self.loss_criterion = nn.MSELoss()\n",
    "        \n",
    "        self.optimizers, self.lr_schedulers = list(), list()\n",
    "        \n",
    "        for idx in range(0,4):\n",
    "            optim = torch.optim.Adam(self.model.c2d_ae[idx].parameters(), lr = learning_rate)\n",
    "            self.optimizers.append(optim)\n",
    "            self.lr_schedulers.append(torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor = LRDecay, patience = LRPatience, threshold = LRThreshold))\n",
    "            del optim\n",
    "            \n",
    "        with open(os.path.join(self.save_path, \"model.txt\"), \"w\") as f:\n",
    "            f.write(str(self.model))\n",
    "        if self.debug: print(\"Model Ready\", \"\\n\", \"-\"*20)\n",
    "        \n",
    "    def epoch_reset(self,):\n",
    "        self.history[\"train_loss\"].append(np.mean(self.epoch_train_loss))\n",
    "        self.history[\"validation_loss\"].append(np.mean(self.epoch_validation_loss))\n",
    "#         for idx in range(0,4):\n",
    "#             self.lr_schedulers[idx].step(np.mean(self.epoch_validation_loss))\n",
    "        self.epoch_train_loss = list()\n",
    "        self.epoch_validation_loss = list()\n",
    "        \n",
    "    def epoch_status(self,):\n",
    "        if not self.stopTraining:            \n",
    "            print(\"Model:\", self.model_type)\n",
    "            try:\n",
    "                d = {\n",
    "                \"Training\" : {\"Loss -2\": self.history[\"train_loss\"][-3],\n",
    "                            \"Loss -1\": self.history[\"train_loss\"][-2],\n",
    "                            \"Loss *\" : self.history[\"train_loss\"][-1],\n",
    "                             },\n",
    "                \"Validation\" : {\"Loss -2\": self.history[\"validation_loss\"][-3],\n",
    "                                \"Loss -1\": self.history[\"validation_loss\"][-2],\n",
    "                                \"Loss *\" : self.history[\"validation_loss\"][-1],\n",
    "                             },\n",
    "            }\n",
    "            except:\n",
    "                d = {\n",
    "                \"Training\" : {\"Loss\" : self.history[\"train_loss\"][-1],},\n",
    "                \"Validation\" : {\"Loss\" : self.history[\"validation_loss\"][-1],},\n",
    "                }\n",
    "            print(pd.DataFrame(d).T)\n",
    "            print(\"-\"*40)\n",
    "        \n",
    "    def save(self,):\n",
    "        save_model(self.model, self.model_file)\n",
    "    \n",
    "    def save_final(self,):\n",
    "        save_model(self.model, \"_FINAL.pth\".join(self.model_file.split(\".pth\")))\n",
    "        plot_stat(self.history, self.model_type, self.save_path)\n",
    "        with open(os.path.join(self.save_path, \"train_stats.pkl\"), \"wb\") as f:\n",
    "            pkl.dump(self.history, f)\n",
    "\n",
    "#     def train_step(self, images):\n",
    "#         self.model.train()\n",
    "#         reconstructions, encodings = self.model(images)\n",
    "\n",
    "#         layer_inputs = images\n",
    "#         losses = list()\n",
    "#         for idx in range(4):\n",
    "#             self.optimizers[idx].zero_grad()\n",
    "#             losses.append(self.loss_criterion(layer_inputs, reconstructions[idx]))\n",
    "#             losses[idx].backward()\n",
    "#             self.optimizers[idx].step()\n",
    "#             layer_inputs = encodings[idx]\n",
    "        \n",
    "#         self.epoch_train_loss.append(torch.sum(torch.stack(losses)).item())\n",
    "        \n",
    "    def train_step(self, images):\n",
    "        self.model.train()\n",
    "        reconstructions, encodings = self.model(images)\n",
    "\n",
    "        layer_inputs = images\n",
    "        total_loss = 0.0\n",
    "        for idx in range(4):\n",
    "            self.optimizers[idx].zero_grad()\n",
    "            total_loss = total_loss + self.loss_criterion(layer_inputs, reconstructions[idx])\n",
    "            layer_inputs = encodings[idx]\n",
    "        \n",
    "        total_loss.backward(retain_graph = True)\n",
    "        for idx in range(4):\n",
    "            self.optimizers[idx].step()\n",
    "\n",
    "        self.epoch_train_loss.append(total_loss.item())    \n",
    "\n",
    "    def val_step(self, images):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            self.model.isTrain = False\n",
    "            reconstructions, encodings = self.model(images)\n",
    "            self.model.isTrain = True\n",
    "            loss = self.loss_criterion(images, reconstructions)\n",
    "        self.epoch_validation_loss.append(loss.item())\n",
    "        \n",
    "    def self_destruct(self):\n",
    "        try: del self.model, self.optimizer, self.loss_criterion, self.history\n",
    "        except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pandora_Trainer:\n",
    "    def __init__(self,\n",
    "                 train_loader,\n",
    "                 val_loader,\n",
    "                 channels,\n",
    "                 learning_rate = 1e-3,\n",
    "                 batch_size = 64,\n",
    "                 epochs = 100,\n",
    "                 LRDecay = 0.75,\n",
    "                 LRPatience = 4,\n",
    "                 LRThreshold = 5e-4,\n",
    "                 num_workers = 4, \n",
    "                 val_split = 0.1,\n",
    "                 status_rate = 5,\n",
    "                 useGPU = True,\n",
    "                 debug = True):\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.status_rate = status_rate\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if useGPU and torch.cuda.is_available(): self.device = torch.device(\"cuda:0\")\n",
    "        \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.channels = channels\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.models = [PandoraModel()]\n",
    "            \n",
    "    def train(self):\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            epoch_st = time()\n",
    "            \n",
    "            # Train\n",
    "            for train_batch_idx, train_batch in enumerate(self.train_loader):\n",
    "                train_images, train_labels = train_batch\n",
    "                train_images = train_images.to(self.device)\n",
    "                for model in self.models:\n",
    "                    if model.stopTraining: continue\n",
    "                    model.train_step(train_images)\n",
    "                \n",
    "            # Validation\n",
    "            for val_batch_idx, val_batch in enumerate(self.val_loader):\n",
    "                val_images, val_labels = val_batch\n",
    "                val_images = val_images.to(self.device)\n",
    "                for model in self.models:\n",
    "                    if model.stopTraining: continue\n",
    "                    model.val_step(val_images)\n",
    "            \n",
    "            # Print Epoch stats\n",
    "            print_status = epoch % self.status_rate == 0 or epoch == 1\n",
    "            if print_status:\n",
    "                print(\"-\"*60)\n",
    "                print(\"Epoch: [%03d/%03d] | time/epoch: %0.2f seconds\"%(epoch, self.epochs, (time() - epoch_st)))\n",
    "                print(\"-\"*60)\n",
    "                \n",
    "            for model in self.models:\n",
    "                if model.stopTraining: continue\n",
    "                model.epoch_reset()\n",
    "                if print_status: model.epoch_status()\n",
    "        \n",
    "        # Finally save models\n",
    "        model_paths = list()\n",
    "        for model in self.models:\n",
    "            model.save_final()\n",
    "            model_paths.append(model.save_path)\n",
    "        \n",
    "        # Clear memory\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except Exception as e:\n",
    "            print(\"Could not clear the memory. Kill the process manually.\")\n",
    "            print(e)\n",
    "            \n",
    "        return model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [00:03,  9.00it/s]\n"
     ]
    }
   ],
   "source": [
    "ucsd = UCSD(sample_stride=1)\n",
    "train_loader, val_loader = get_data_loaders(ucsd, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandora_c2d_AE READY\n",
      "Model Ready \n",
      " --------------------\n"
     ]
    }
   ],
   "source": [
    "trainer = Pandora_Trainer(train_loader, val_loader, 1, 5e-4, 32, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch: [001/100] | time/epoch: 131.22 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "                Loss\n",
      "Training    0.675867\n",
      "Validation  0.008440\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [005/100] | time/epoch: 135.65 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.148723  0.072377  0.026511\n",
      "Validation  0.002046  0.001385  0.000988\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [010/100] | time/epoch: 143.41 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.008675  0.010563  0.005337\n",
      "Validation  0.000666  0.000956  0.000598\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [015/100] | time/epoch: 143.77 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.002333  0.002100  0.002132\n",
      "Validation  0.000493  0.000471  0.000494\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [020/100] | time/epoch: 131.93 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.001551  0.001336  0.001271\n",
      "Validation  0.000400  0.000488  0.000412\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [025/100] | time/epoch: 143.63 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.001066  0.001021  0.000984\n",
      "Validation  0.000390  0.000377  0.000401\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [030/100] | time/epoch: 143.56 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000848  0.000901  0.001100\n",
      "Validation  0.000376  0.000369  0.000402\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [035/100] | time/epoch: 130.71 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000844  0.000729  0.000692\n",
      "Validation  0.000392  0.000387  0.000379\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [040/100] | time/epoch: 143.22 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000593  0.000573  0.000599\n",
      "Validation  0.000364  0.000352  0.000386\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [045/100] | time/epoch: 143.25 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2  Loss -1    Loss *\n",
      "Training    0.000514  0.00050  0.000559\n",
      "Validation  0.000347  0.00038  0.000338\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [050/100] | time/epoch: 143.60 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000452  0.000437  0.000572\n",
      "Validation  0.000333  0.000353  0.000351\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [055/100] | time/epoch: 134.27 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000456  0.000404  0.000398\n",
      "Validation  0.000345  0.000346  0.000332\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [060/100] | time/epoch: 143.78 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000389  0.000384  0.000373\n",
      "Validation  0.000322  0.000329  0.000317\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [065/100] | time/epoch: 143.32 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000431  0.000362  0.000352\n",
      "Validation  0.000314  0.000319  0.000320\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [070/100] | time/epoch: 130.98 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000341  0.000345  0.000342\n",
      "Validation  0.000314  0.000306  0.000351\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [075/100] | time/epoch: 144.33 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000340  0.000359  0.000513\n",
      "Validation  0.000323  0.000316  0.000380\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [080/100] | time/epoch: 143.93 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000700  0.000438  0.000391\n",
      "Validation  0.000397  0.000394  0.000317\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [085/100] | time/epoch: 131.52 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000352  0.000339  0.000333\n",
      "Validation  0.000315  0.000308  0.000304\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [090/100] | time/epoch: 144.99 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000334  0.000325  0.000322\n",
      "Validation  0.000312  0.000297  0.000301\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [095/100] | time/epoch: 145.00 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000318  0.000314  0.000321\n",
      "Validation  0.000294  0.000366  0.000298\n",
      "----------------------------------------\n",
      "------------------------------------------------------------\n",
      "Epoch: [100/100] | time/epoch: 135.31 seconds\n",
      "------------------------------------------------------------\n",
      "Model: pandora_c2d_AE\n",
      "             Loss -2   Loss -1    Loss *\n",
      "Training    0.000314  0.000312  0.000318\n",
      "Validation  0.000289  0.000305  0.000290\n",
      "----------------------------------------\n",
      "'val_loss'\n",
      "Could not clear the memory. Kill the process manually.\n",
      "name 'gc' is not defined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/pandora_c2d_AE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [00:02,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [00:04,  8.29it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = UCSD(1, isTrain=False, sample_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoEncoders.test_autoencoder import AutoEncoder_Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.PandoraModel at 0x7ff9248e4588>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/pandora_c2d_AE/pandora_c2d_AE.pth.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:10,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:38,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:41,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [00:43,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'list' object has no attribute 'detach'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [01:03,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.537123989385602\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ae_model in trainer.models:\n",
    "    ae_model.isTrain = False\n",
    "    print(ae_model.model_file)\n",
    "    tester = AutoEncoder_Tester(\n",
    "        ae_model.model,\n",
    "        test_data\n",
    "    )\n",
    "    score = tester.test(save_as = \".pkl\".join(ae_model.model_file.split(\".pth.tar\")))\n",
    "    print(score)\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
